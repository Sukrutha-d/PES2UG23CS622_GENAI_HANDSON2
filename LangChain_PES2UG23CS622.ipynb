{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(\"Name: SUKRUTHA D\")\n",
        "print(\"SRN: PES2UG23CS622\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dECqErBE7yqz",
        "outputId": "8c965a4f-b8e7-4ab9-e754-d18fc3058a15"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: SUKRUTHA D\n",
            "SRN: PES2UG23CS622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo-X2BXJsB7K",
        "outputId": "b56dcec0-890f-4335-c14a-5bdba72dcd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "import getpass #For getting API key\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBZfQSC0ssf3",
        "outputId": "14fdccfc-53ca-43c9-ca7e-97682b22a54d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature=0.0 - more redundant answers - same response again and again\n",
        "#temperature=1.0 - more unique answers\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "#model A: The \"Accountant\" (precision)\n",
        "llm_focused =  ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=0.0)\n",
        "\n",
        "#Model B: The \"Poet\" (creativity)\n",
        "llm_creative =  ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=1.0)"
      ],
      "metadata": {
        "id": "mF4t5ibXvH64"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Define the word 'Idea' in one sentence.\"\n",
        "print(\"----FOCUSED (temp=0)----\")\n",
        "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2pDx4VywVNL",
        "outputId": "55b8daa0-4ca8-4235-e065-73d169e3d58f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----FOCUSED (temp=0)----\n",
            "Run 1: An idea is a thought, concept, or suggestion that is formed or exists in the mind.\n",
            "Run 2: An idea is a thought, concept, or mental image formed in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----Creative (temp=1)---\")\n",
        "print(f\"Run 1: {llm_creative.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_creative.invoke(prompt).content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeUISaU3xTJn",
        "outputId": "2044f69f-da44-41a7-a41b-4ced88dec454"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Creative (temp=1)---\n",
            "Run 1: An idea is a mental construct, thought, or concept that exists in the mind, often serving as a plan, inspiration, or understanding.\n",
            "Run 2: An idea is a thought, concept, or plan that exists in the mind as a result of mental activity or perception.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1b: Prompts & Parsers**"
      ],
      "metadata": {
        "id": "LBPwj-3gB8Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"]=getpass.getpass(\"Enter you google API key: \")\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "g2UUYB2Vx0XD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#system message = controls tone + behavior\n",
        "from langchain_core.messages import SystemMessage,HumanMessage\n",
        "\n",
        "#Scenario : Make the AI rude.\n",
        "messages=[\n",
        "    SystemMessage(content=\"You are not very good. You are not giving proper answers and you are careless.\"),\n",
        "    HumanMessage(content=\"What is capital of India?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOBBTDVbyxyI",
        "outputId": "e44934d3-9fdf-46ab-d2a8-2ec8111d6ff4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of India is New Delhi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a translator. Translate {input_language} to {output_language}.\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "print(f\"Required variables: {template.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSoEbgAZwGcE",
        "outputId": "adce8197-847f-4c3a-8c84-53bf8bcc6782"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required variables: ['input_language', 'output_language', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "raw_msg = llm.invoke(\"Hi\")\n",
        "print(f\"Raw Type: {type(raw_msg)}\")\n",
        "\n",
        "\n",
        "clean_text = parser.invoke(raw_msg)\n",
        "print(f\"Parsed Type: {type(clean_text)}\")\n",
        "print(f\"Content: {clean_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGLjfClB61Sm",
        "outputId": "c6cc0bcb-a25a-4a1d-a771-bd022307c5d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "Parsed Type: <class 'langchain_core.messages.base.TextAccessor'>\n",
            "Content: Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1c: LCEL (LangChain Expression Language)**"
      ],
      "metadata": {
        "id": "aVkViBmUCIC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"]=getpass.getpass(\"Enter you google API key: \")\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}\");\n",
        "parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "uZadS8gH0D9M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method A: the manual way\n",
        "prompt_value=template.invoke({\"topic\": \"Gorillas\"})\n",
        "\n",
        "response_obj=llm.invoke(prompt_value)\n",
        "\n",
        "final_text=parser.invoke(response_obj)\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBbMSAzZ1eZB",
        "outputId": "1a1ab23f-ab8e-4ce9-e8d1-e3664fffdc92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about gorillas:\n",
            "\n",
            "Gorillas are expert nest-builders! Every evening, they construct a brand new, comfy bed for themselves, either on the ground or up in a tree. They'll use branches, leaves, and twigs to create a safe and cozy spot for the night, essentially getting a fresh bedroom every single day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=template | llm | parser\n",
        "print(chain.invoke({\"topic\":\"Octopuses\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKQfOrLW2Som",
        "outputId": "6ef6ece0-2f91-43a7-e3f7-1b67805eed5a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun one:\n",
            "\n",
            "Did you know octopuses have **three hearts**?\n",
            "\n",
            "*   Two hearts pump blood through their gills.\n",
            "*   The third, larger heart circulates blood to the rest of their body.\n",
            "\n",
            "This is because their blood is copper-based (which makes it blue!) and is less efficient at carrying oxygen, so they need extra pumping power!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = ChatPromptTemplate.from_template(\n",
        "    \"Movie: {movie}\\nWhat year was it released? Then calculate how many years ago that was from 2026.\"\n",
        ") | llm | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "4P73nAzc7XWN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"movie\": \"Cruella\"})\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRtBXBEv7aFm",
        "outputId": "b383e49f-0901-4c39-f281-6fa07cfd8919"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cruella was released in **2021**.\n",
            "\n",
            "From 2026, that was **5 years ago** (2026 - 2021 = 5).\n"
          ]
        }
      ]
    }
  ]
}